---
globs: lib/backend.ts,lib/engines/*/backend.ts
description: Backend integration and API patterns
---

# Backend Integration

## Backend Architecture

The backend system provides a unified interface for LLM integration and API abstraction:

- **Backend Interface**: [lib/backend.ts](mdc:lib/backend.ts) - Core backend interface
- **Default Backend**: OpenAI integration with streaming support
- **Engine-Specific Backends**: Custom backends for different engines

## Backend Interface

```typescript
export interface Backend {
  getNarration(prompt: Prompt, onToken?: TokenCallback): Promise<string>;
  getObject<Schema extends z.ZodType, Type extends z.infer<Schema>>(
    prompt: Prompt,
    schema: Schema,
    onToken?: TokenCallback,
  ): Promise<Type>;
  abort(): void;
  isAbortError(error: unknown): boolean;
}
```

## Streaming Implementation

### Key Principles
- **Backend Trust Pattern**: Always trust the backend to accumulate complete responses
- **Immer Compliance**: Never mutate Immer drafts inside async callbacks
- **Progress Updates**: Use `onToken` callback for real-time progress

### Correct Pattern
```typescript
// ✅ CORRECT - Trust backend accumulation
event.text = await backend.getNarration(narratePrompt(state, action), (token: string, count: number) => {
  onToken(token, count);        // Handle progress updates
  updateState();               // Update state during streaming
});
// event.text now contains complete response from backend
```

### Anti-Patterns to Avoid
```typescript
// ❌ WRONG - Double accumulation
let streamingText = "";
event.text = await backend.getNarration(prompt, (token: string, count: number) => {
  streamingText += token;      // Manual accumulation
  event.text = streamingText;  // Draft mutation in callback
  onToken(token, count);
  updateState();
});
event.text = streamingText;    // Overwriting backend result
```

## Default Backend Implementation

### OpenAI Integration
- Uses OpenAI client with streaming support
- Handles abort signals and error recovery
- Accumulates complete responses internally
- Provides progress callbacks for UI updates

### Configuration
- API URL, key, and model from environment variables
- Generation and narration parameters
- Request timeout and retry logic

## Error Handling

### Abort Errors
- Use `isAbortError()` to detect user cancellations
- Handle `AbortController` signals properly
- Reset controller after abort for reuse

### API Errors
- Handle OpenAI API errors gracefully
- Provide user-friendly error messages
- Log errors for debugging

## Development Guidelines

### Adding New Backends
1. Implement the `Backend` interface
2. Handle streaming and error cases
3. Register in the backend system
4. Test with different engines

### Testing Backends
- Test streaming functionality
- Verify error handling
- Test abort scenarios
- Validate response accumulation

## Environment Configuration

### Required Variables
- `OPENAI_API_URL` - API endpoint
- `OPENAI_API_KEY` - Authentication key
- `OPENAI_MODEL` - Model to use

### Optional Variables
- `OPENAI_GENERATION_PARAMS` - Generation parameters
- `OPENAI_NARRATION_PARAMS` - Narration parameters